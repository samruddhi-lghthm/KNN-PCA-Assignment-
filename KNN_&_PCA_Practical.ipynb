{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a KNN Classifier on the Iris dataset and print model accuracy."
      ],
      "metadata": {
        "id": "QYICbqCrkxfC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPBCd-ZMkqMD"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "tF1CtJEZlek-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=200, n_features=1, noise=10, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the KNN Regressor\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
        "\n",
        "# Train the model\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn_regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "VunhqfCXlcO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy."
      ],
      "metadata": {
        "id": "Lpnl0WJKl254"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# KNN with Euclidean distance (default)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# KNN with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=3, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy with Euclidean distance: {accuracy_euclidean * 100:.2f}%\")\n",
        "print(f\"Accuracy with Manhattan distance: {accuracy_manhattan * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "XV4xRVYclt6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a KNN Classifier with different values of K and visualize decision boundaries"
      ],
      "metadata": {
        "id": "2re5hEtxmXq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn import datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Only take the first two features for 2D plotting\n",
        "y = iris.target\n",
        "h = .02  # step size in the mesh\n",
        "\n",
        "# Create color maps\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
        "cmap_bold  = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Try K values from 1 to 5\n",
        "k_values = [1, 3, 5]\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "for i, k in enumerate(k_values, 1):\n",
        "    # Create KNN classifier and fit\n",
        "    clf = KNeighborsClassifier(n_neighbors=k)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Create mesh grid\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Predict on mesh grid\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot\n",
        "    plt.subplot(1, len(k_values), i)\n",
        "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold, edgecolor='k', s=40, label=\"Train\")\n",
        "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, edgecolor='k', s=100, marker='*', label=\"Test\")\n",
        "    plt.title(f\"K = {k}\")\n",
        "    plt.xlabel(\"Sepal length\")\n",
        "    plt.ylabel(\"Sepal width\")\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2zHQ6ldAmUTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Apply Feature Scaling before training a KNN model and compare results with unscaled data."
      ],
      "metadata": {
        "id": "AAA_XKs7msrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- KNN without Feature Scaling ---\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# --- Apply Feature Scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- KNN with Feature Scaling ---\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# --- Compare Results ---\n",
        "print(f\"Accuracy without Scaling: {accuracy_unscaled * 100:.2f}%\")\n",
        "print(f\"Accuracy with Scaling:    {accuracy_scaled * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "iFBJGd-nmrIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a PCA model on synthetic data and print the explained variance ratio for each component."
      ],
      "metadata": {
        "id": "7FZhveGHnQTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic dataset with 5 features\n",
        "X, y = make_classification(n_samples=200, n_features=5, n_informative=3, random_state=42)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Print explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "for i, ratio in enumerate(explained_variance):\n",
        "    print(f\"Principal Component {i+1}: {ratio:.4f} ({ratio * 100:.2f}% of variance explained)\")\n"
      ],
      "metadata": {
        "id": "Extycr_hnL-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA."
      ],
      "metadata": {
        "id": "aiMLbB4dnjDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ----- KNN without PCA -----\n",
        "knn_no_pca = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_no_pca.fit(X_train_scaled, y_train)\n",
        "y_pred_no_pca = knn_no_pca.predict(X_test_scaled)\n",
        "accuracy_no_pca = accuracy_score(y_test, y_pred_no_pca)\n",
        "\n",
        "# ----- Apply PCA -----\n",
        "pca = PCA(n_components=2)  # Reduce to 2 components\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# ----- KNN with PCA -----\n",
        "knn_with_pca = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_with_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_with_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# ----- Results -----\n",
        "print(f\"Accuracy without PCA: {accuracy_no_pca * 100:.2f}%\")\n",
        "print(f\"Accuracy with PCA (2 components): {accuracy_pca * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "IyBLr1Pmnham"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV"
      ],
      "metadata": {
        "id": "rF6uAISgn1TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
        "    'weights': ['uniform', 'distance']\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters from GridSearchCV\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Train the best model and evaluate on test set\n",
        "best_knn = grid_search.best_estimator_\n",
        "y_pred = best_knn.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the best KNN model: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "2mnX7NChnwhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a KNN Classifier and check the number of misclassified samples."
      ],
      "metadata": {
        "id": "i0VpbQJ_oami"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate number of misclassified samples\n",
        "misclassified = (y_test != y_pred).sum()\n",
        "\n",
        "print(f\"Number of misclassified samples: {misclassified}\")\n"
      ],
      "metadata": {
        "id": "psk82JAroEP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train a PCA model and visualize the cumulative explained variance."
      ],
      "metadata": {
        "id": "LW-SeSaapO7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Iris dataset (or any other dataset with multiple features)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Standardize the data\n",
        "# PCA is sensitive to the scale of the features, so it's important to standardize.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "# n_components=None means that PCA will keep all original components\n",
        "pca = PCA(n_components=None)\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Get the explained variance ratio for each component\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Calculate the cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Visualize the cumulative explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='-')\n",
        "plt.title('Cumulative Explained Variance by Principal Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.xticks(range(1, len(cumulative_explained_variance) + 1))\n",
        "plt.yticks(np.arange(0, 1.1, 0.1))\n",
        "plt.show()\n",
        "\n",
        "print(\"Explained variance ratio of each component:\", explained_variance_ratio)\n",
        "print(\"Cumulative explained variance:\", cumulative_explained_variance)"
      ],
      "metadata": {
        "id": "CvJtvd6MpC2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare\n",
        "accuracy."
      ],
      "metadata": {
        "id": "Vb-5nxGgPxZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset (a common dataset for classification)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the number of neighbors (k) to use\n",
        "n_neighbors = 5\n",
        "\n",
        "# --- Train and evaluate KNN with uniform weights ---\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=n_neighbors, weights='uniform')\n",
        "knn_uniform.fit(X_train, y_train)\n",
        "y_pred_uniform = knn_uniform.predict(X_test)\n",
        "accuracy_uniform = accuracy_score(y_test, y_pred_uniform)\n",
        "\n",
        "print(f\"KNN with uniform weights (k={n_neighbors}):\")\n",
        "print(f\"  Accuracy on the test set: {accuracy_uniform:.4f}\")\n",
        "\n",
        "# --- Train and evaluate KNN with distance weights ---\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=n_neighbors, weights='distance')\n",
        "knn_distance.fit(X_train, y_train)\n",
        "y_pred_distance = knn_distance.predict(X_test)\n",
        "accuracy_distance = accuracy_score(y_test, y_pred_distance)\n",
        "\n",
        "print(f\"\\nKNN with distance weights (k={n_neighbors}):\")\n",
        "print(f\"  Accuracy on the test set: {accuracy_distance:.4f}\")\n",
        "\n",
        "# --- Comparison ---\n",
        "print(\"\\nComparison:\")\n",
        "if accuracy_uniform > accuracy_distance:\n",
        "    print(f\"Uniform weights performed better with an accuracy of {accuracy_uniform:.4f} compared to distance weights ({accuracy_distance:.4f}).\")\n",
        "elif accuracy_distance > accuracy_uniform:\n",
        "    print(f\"Distance weights performed better with an accuracy of {accuracy_distance:.4f} compared to uniform weights ({accuracy_uniform:.4f}).\")\n",
        "else:\n",
        "    print(f\"Both uniform and distance weights achieved the same accuracy of {accuracy_uniform:.4f}.\")"
      ],
      "metadata": {
        "id": "ihPWQ1HNP4FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a KNN Regressor and analyze the effect of different K values on performance."
      ],
      "metadata": {
        "id": "4jIIkRcYR3VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=200, n_features=1, noise=20, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define a range of K values to test\n",
        "k_values = range(1, 21)  # Test K from 1 to 20\n",
        "\n",
        "# Store the performance metrics for each K\n",
        "mse_scores = []\n",
        "r2_scores = []\n",
        "\n",
        "# Train and evaluate KNN Regressor for each K\n",
        "for k in k_values:\n",
        "    # Initialize the KNN Regressor with the current K\n",
        "    knn_regressor = KNeighborsRegressor(n_neighbors=k)\n",
        "\n",
        "    # Train the model\n",
        "    knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = knn_regressor.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Store the results\n",
        "    mse_scores.append(mse)\n",
        "    r2_scores.append(r2)\n",
        "\n",
        "# --- Visualize the effect of K on Mean Squared Error (MSE) ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(k_values, mse_scores, marker='o', linestyle='-', color='red')\n",
        "plt.title('Effect of K on Mean Squared Error')\n",
        "plt.xlabel('Number of Neighbors (K)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.grid(True)\n",
        "plt.xticks(k_values)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Visualize the effect of K on R-squared (R²) ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(k_values, r2_scores, marker='o', linestyle='-', color='green')\n",
        "plt.title('Effect of K on R-squared (R²)')\n",
        "plt.xlabel('Number of Neighbors (K)')\n",
        "plt.ylabel('R-squared (R²)')\n",
        "plt.grid(True)\n",
        "plt.xticks(k_values)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Print the performance metrics for each K (optional) ---\n",
        "print(\"Performance metrics for different K values:\")\n",
        "for i, k in enumerate(k_values):\n",
        "    print(f\"K={k}: MSE={mse_scores[i]:.4f}, R²={r2_scores[i]:.4f}\")\n",
        "\n",
        "# --- Using Cross-Validation for more robust K selection ---\n",
        "print(\"\\n--- Analyzing K using Cross-Validation ---\")\n",
        "cv_scores = []\n",
        "for k in k_values:\n",
        "    knn = KNeighborsRegressor(n_neighbors=k)\n",
        "    # Perform 5-fold cross-validation, scoring by negative mean squared error\n",
        "    # We use negative MSE because cross_val_score aims to maximize the score\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "    # Convert negative MSE back to positive and take the mean\n",
        "    mse_cv = -scores.mean()\n",
        "    cv_scores.append(mse_cv)\n",
        "\n",
        "# Find the optimal K based on cross-validation\n",
        "optimal_k_cv = k_values[np.argmin(cv_scores)]\n",
        "print(f\"Optimal K (based on cross-validation MSE): {optimal_k_cv}\")\n",
        "\n",
        "# Visualize cross-validation MSE\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(k_values, cv_scores, marker='o', linestyle='-', color='blue')\n",
        "plt.title('Cross-Validation Mean Squared Error vs. K')\n",
        "plt.xlabel('Number of Neighbors (K)')\n",
        "plt.ylabel('Cross-Validation Mean Squared Error')\n",
        "plt.grid(True)\n",
        "plt.xticks(k_values)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Train the final model with the optimal K found by cross-validation\n",
        "final_knn = KNeighborsRegressor(n_neighbors=optimal_k_cv)\n",
        "final_knn.fit(X_train, y_train)\n",
        "final_y_pred = final_knn.predict(X_test)\n",
        "final_mse = mean_squared_error(y_test, final_y_pred)\n",
        "final_r2 = r2_score(y_test, final_y_pred)\n",
        "\n",
        "print(f\"\\nPerformance of KNN Regressor with optimal K ({optimal_k_cv}):\")\n",
        "print(f\"  MSE on Test Set: {final_mse:.4f}\")\n",
        "print(f\"  R² on Test Set: {final_r2:.4f}\")"
      ],
      "metadata": {
        "id": "X5JBPpvrRP8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Implement KNN Imputation for handling missing values in a dataset."
      ],
      "metadata": {
        "id": "C_AaXCmCS5ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Create a sample dataset with missing values\n",
        "data = {\n",
        "    'A': [1, 2, np.nan, 4, 5, np.nan, 7, 8],\n",
        "    'B': [np.nan, 10, 11, 12, np.nan, 14, 15, 16],\n",
        "    'C': [20, 21, 22, np.nan, 24, 25, np.nan, 27],\n",
        "    'D': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Original DataFrame with Missing Values:\")\n",
        "print(df)\n",
        "\n",
        "# 2. Separate features with missing values from those without (optional but good practice)\n",
        "missing_cols = df.columns[df.isnull().any()].tolist()\n",
        "non_missing_cols = df.columns[df.notnull().all()].tolist()\n",
        "\n",
        "df_missing = df[missing_cols].copy()\n",
        "df_non_missing = df[non_missing_cols].copy()\n",
        "\n",
        "# 3. Initialize the KNNImputer\n",
        "# n_neighbors: The number of neighboring samples to use for imputation.\n",
        "# weights: Weight function used in prediction. Possible values:\n",
        "#          - 'uniform': uniform weights. All neighbors weights equally.\n",
        "#          - 'distance': weight points by the inverse of their distance.\n",
        "# missing_values: The placeholder for the missing values. Defaults to np.nan.\n",
        "imputer = KNNImputer(n_neighbors=3, weights='uniform')\n",
        "\n",
        "# 4. Fit the imputer to the features with missing values and transform\n",
        "imputed_array = imputer.fit_transform(df_missing)\n",
        "\n",
        "# 5. Create a new DataFrame with the imputed values\n",
        "df_imputed = pd.DataFrame(imputed_array, columns=df_missing.columns)\n",
        "print(\"\\nDataFrame with KNN Imputation (n_neighbors=3, uniform weights):\")\n",
        "print(df_imputed)\n",
        "\n",
        "# 6. Combine the imputed features with the non-missing features (if any)\n",
        "if not df_non_missing.empty:\n",
        "    df_final = pd.concat([df_imputed, df_non_missing], axis=1)\n",
        "else:\n",
        "    df_final = df_imputed\n",
        "\n",
        "print(\"\\nFinal DataFrame after Imputation:\")\n",
        "print(df_final)\n",
        "\n",
        "# 7. Analyze the effect of different K values (optional)\n",
        "print(\"\\n--- Analyzing the effect of different K values ---\")\n",
        "k_values = [1, 3, 5, 7]\n",
        "imputed_dfs = {}\n",
        "\n",
        "for k in k_values:\n",
        "    imputer_k = KNNImputer(n_neighbors=k, weights='uniform')\n",
        "    imputed_array_k = imputer_k.fit_transform(df_missing)\n",
        "    imputed_dfs[f'K={k}'] = pd.DataFrame(imputed_array_k, columns=df_missing.columns)\n",
        "    print(f\"\\nDataFrame with KNN Imputation (n_neighbors={k}, uniform weights):\")\n",
        "    print(imputed_dfs[f'K={k}'])\n",
        "\n",
        "# 8. (More Advanced Analysis - Requires a \"True\" dataset for comparison)\n",
        "#    To truly analyze the effect of different K values, you would ideally\n",
        "#    have a dataset where you artificially introduce missing values and\n",
        "#    then compare the imputed values with the original true values.\n",
        "\n",
        "# Example of advanced analysis (requires a complete dataset):\n",
        "print(\"\\n--- Advanced Analysis (Requires a complete dataset) ---\")\n",
        "# Assume you have a complete dataset 'df_complete'\n",
        "# Introduce some artificial missing values in 'df_complete' to create 'df_missing_artificial'\n",
        "if 'df_complete' in locals():  # Only run if df_complete is defined\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "\n",
        "    # Let's just use our original 'df' and pretend it's complete for this example\n",
        "    df_complete = df.copy()\n",
        "    df_missing_artificial = df_complete.copy()\n",
        "    missing_indices = [(0, 'B'), (2, 'A'), (3, 'C'), (4, 'B'), (6, 'C')]\n",
        "    for row, col in missing_indices:\n",
        "        df_missing_artificial.loc[row, col] = np.nan\n",
        "\n",
        "    print(\"\\nArtificially Created Missing Data:\")\n",
        "    print(df_missing_artificial)\n",
        "\n",
        "    true_values = {}\n",
        "    for row, col in missing_indices:\n",
        "        true_values[(row, col)] = df_complete.loc[row, col]\n",
        "\n",
        "    mse_per_k = {}\n",
        "    k_values_analysis = [1, 3, 5, 7]\n",
        "\n",
        "    for k in k_values_analysis:\n",
        "        imputer_analysis = KNNImputer(n_neighbors=k, weights='uniform')\n",
        "        df_imputed_analysis = pd.DataFrame(imputer_analysis.fit_transform(df_missing_artificial),\n",
        "                                            columns=df_missing_artificial.columns)\n",
        "\n",
        "        predictions = []\n",
        "        originals = []\n",
        "        for (row, col), true_val in true_values.items():\n",
        "            predictions.append(df_imputed_analysis.loc[row, col])\n",
        "            originals.append(true_val)\n",
        "\n",
        "        mse = mean_squared_error(originals, predictions)\n",
        "        mse_per_k[f'K={k}'] = mse\n",
        "        print(f\"\\nMean Squared Error for K={k}: {mse}\")\n",
        "\n",
        "    print(\"\\nMSE for different K values (compared to true values):\")\n",
        "    print(mse_per_k)"
      ],
      "metadata": {
        "id": "8x2mXnPkSYSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34.  Train a PCA model and visualize the data projection onto the first two principal components."
      ],
      "metadata": {
        "id": "x7-Gs70ITZIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to reduce to 2 principal components\n",
        "n_components = 2\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Visualize the projected data\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['r', 'g', 'b']\n",
        "lw = 2\n",
        "\n",
        "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
        "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8, lw=lw,\n",
        "                label=target_name)\n",
        "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]*100:.2f}% variance)')\n",
        "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]*100:.2f}% variance)')\n",
        "plt.title('PCA of Iris Dataset onto the First Two Principal Components')\n",
        "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print the explained variance ratio\n",
        "print(\"Explained variance ratio of the first two principal components:\", pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "hbb09FugTXri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance."
      ],
      "metadata": {
        "id": "WEkTdou4UwTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the number of neighbors (k)\n",
        "n_neighbors = 5\n",
        "\n",
        "# --- KNN Classifier with KD Tree ---\n",
        "start_time_kd = time.time()\n",
        "knn_kd_tree = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='kd_tree')\n",
        "knn_kd_tree.fit(X_train, y_train)\n",
        "end_time_kd = time.time()\n",
        "train_time_kd = end_time_kd - start_time_kd\n",
        "\n",
        "start_time_predict_kd = time.time()\n",
        "y_pred_kd = knn_kd_tree.predict(X_test)\n",
        "end_time_predict_kd = time.time()\n",
        "predict_time_kd = end_time_predict_kd - start_time_predict_kd\n",
        "\n",
        "accuracy_kd = accuracy_score(y_test, y_pred_kd)\n",
        "\n",
        "print(\"KNN Classifier with KD Tree:\")\n",
        "print(f\"  Training Time: {train_time_kd:.4f} seconds\")\n",
        "print(f\"  Prediction Time: {predict_time_kd:.4f} seconds\")\n",
        "print(f\"  Accuracy: {accuracy_kd:.4f}\")\n",
        "\n",
        "# --- KNN Classifier with Ball Tree ---\n",
        "start_time_ball = time.time()\n",
        "knn_ball_tree = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='ball_tree')\n",
        "knn_ball_tree.fit(X_train, y_train)\n",
        "end_time_ball = time.time()\n",
        "train_time_ball = end_time_ball - start_time_ball\n",
        "\n",
        "start_time_predict_ball = time.time()\n",
        "y_pred_ball = knn_ball_tree.predict(X_test)\n",
        "end_time_predict_ball = time.time()\n",
        "predict_time_ball = end_time_predict_ball - start_time_predict_ball\n",
        "\n",
        "accuracy_ball = accuracy_score(y_test, y_pred_ball)\n",
        "\n",
        "print(\"\\nKNN Classifier with Ball Tree:\")\n",
        "print(f\"  Training Time: {train_time_ball:.4f} seconds\")\n",
        "print(f\"  Prediction Time: {predict_time_ball:.4f} seconds\")\n",
        "print(f\"  Accuracy: {accuracy_ball:.4f}\")\n",
        "\n",
        "# --- Comparison ---\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"  Accuracy (KD Tree): {accuracy_kd:.4f}\")\n",
        "print(f\"  Accuracy (Ball Tree): {accuracy_ball:.4f}\")\n",
        "print(f\"  Training Time (KD Tree): {train_time_kd:.4f} seconds\")\n",
        "print(f\"  Training Time (Ball Tree): {train_time_ball:.4f} seconds\")\n",
        "print(f\"  Prediction Time (KD Tree): {predict_time_kd:.4f} seconds\")\n",
        "print(f\"  Prediction Time (Ball Tree): {predict_time_ball:.4f} seconds\")\n",
        "\n",
        "if accuracy_kd == accuracy_ball:\n",
        "    print(\"\\nBoth KD Tree and Ball Tree achieved the same accuracy.\")\n",
        "elif accuracy_kd > accuracy_ball:\n",
        "    print(\"\\nKD Tree achieved a higher accuracy.\")\n",
        "else:\n",
        "    print(\"\\nBall Tree achieved a higher accuracy.\")\n",
        "\n",
        "if train_time_kd < train_time_ball:\n",
        "    print(\"KD Tree trained faster.\")\n",
        "elif train_time_ball < train_time_kd:\n",
        "    print(\"Ball Tree trained faster.\")\n",
        "else:\n",
        "    print(\"KD Tree and Ball Tree had similar training times.\")\n",
        "\n",
        "if predict_time_kd < predict_time_ball:\n",
        "    print(\"KD Tree predicted faster.\")\n",
        "elif predict_time_ball < predict_time_kd:\n",
        "    print(\"Ball Tree predicted faster.\")\n",
        "else:\n",
        "    print(\"KD Tree and Ball Tree had similar prediction times.\")"
      ],
      "metadata": {
        "id": "gO2b_JEyUub-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot."
      ],
      "metadata": {
        "id": "T0IkiSENVTtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Step 1: Generate a high-dimensional synthetic dataset\n",
        "X, _ = make_classification(\n",
        "    n_samples=500,        # number of samples\n",
        "    n_features=50,        # total number of features\n",
        "    n_informative=30,     # number of informative features\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Train a PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Step 3: Extract explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Step 4: Plot the Scree plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(\n",
        "    range(1, len(explained_variance_ratio) + 1),\n",
        "    explained_variance_ratio,\n",
        "    marker='o', linestyle='-'\n",
        ")\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.xticks(range(1, len(explained_variance_ratio) + 1, 2))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8XK7xcZFYUQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score."
      ],
      "metadata": {
        "id": "eTWtS0w0bVN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Step 1: Generate a synthetic classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
        "    n_classes=2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train a KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate using precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Optional: Full classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display metrics\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "cXiS9S6BbPSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a PCA model and analyze the effect of different numbers of components on accuracy."
      ],
      "metadata": {
        "id": "TE-iZY1bcysV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Generate a synthetic high-dimensional classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=50, n_informative=30, n_classes=2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Evaluate accuracy using different numbers of PCA components\n",
        "components_range = range(1, 51)\n",
        "accuracies = []\n",
        "\n",
        "for n_components in components_range:\n",
        "    # Apply PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "\n",
        "    # Train KNN classifier\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(X_train_pca, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = knn.predict(X_test_pca)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Step 4: Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(components_range, accuracies, marker='o')\n",
        "plt.title('Effect of PCA Components on Classification Accuracy')\n",
        "plt.xlabel('Number of PCA Components')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PVxoYxn5cxfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a KNN Classifier with different leaf_size values and compare accuracy."
      ],
      "metadata": {
        "id": "nPjUuwVOdfY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Generate a classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=15, n_classes=2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Try different leaf_size values\n",
        "leaf_sizes = range(5, 55, 5)\n",
        "accuracies = []\n",
        "\n",
        "for leaf_size in leaf_sizes:\n",
        "    # Train KNN with varying leaf_size\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, leaf_size=leaf_size)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = knn.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Step 4: Plot accuracy vs leaf_size\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(leaf_sizes, accuracies, marker='o', linestyle='-')\n",
        "plt.title('KNN Accuracy vs. Leaf Size')\n",
        "plt.xlabel('Leaf Size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Yl5SUzcSdcpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a PCA model and visualize how data points are transformed before and after PCA."
      ],
      "metadata": {
        "id": "pAF7y34WeMno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Generate a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=500, n_features=10, n_informative=5, n_classes=2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Standardize the data (important before PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Reduce to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Step 4: Plot before PCA (first 2 original features)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\n",
        "plt.title('Original Data (First 2 Features)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid(True)\n",
        "\n",
        "# Step 5: Plot after PCA\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\n",
        "plt.title('Data After PCA (2 Components)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TK9niqnOeCWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report."
      ],
      "metadata": {
        "id": "YCl7WaAyefky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Make predictions and print classification report\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "report = classification_report(y_test, y_pred, target_names=wine.target_names)\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "aAlw23X4eck7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error."
      ],
      "metadata": {
        "id": "Qe5w6kq5e4oR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Generate a regression dataset\n",
        "X, y = make_regression(\n",
        "    n_samples=500, n_features=10, noise=10, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Try different distance metrics\n",
        "distance_metrics = ['euclidean', 'manhattan', 'chebyshev']\n",
        "mse_scores = []\n",
        "\n",
        "for metric in distance_metrics:\n",
        "    # Train KNN Regressor with given distance metric\n",
        "    knn_reg = KNeighborsRegressor(n_neighbors=5, metric=metric)\n",
        "    knn_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict and compute MSE\n",
        "    y_pred = knn_reg.predict(X_test_scaled)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Step 5: Plot the results\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(distance_metrics, mse_scores, color='skyblue')\n",
        "plt.title('Effect of Distance Metric on KNN Regressor Performance')\n",
        "plt.xlabel('Distance Metric')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kFym5ZcJe0Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43.  Train a KNN Classifier and evaluate using ROC-AUC score."
      ],
      "metadata": {
        "id": "-tlnTrL7faoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Create a synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
        "    n_classes=2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Predict probabilities and compute ROC-AUC\n",
        "y_proba = knn.predict_proba(X_test_scaled)[:, 1]  # Probability of positive class\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Step 6: Plot ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('KNN Classifier ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Print ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "KHg_y_WBfObE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44.  Train a PCA model and visualize the variance captured by each principal component."
      ],
      "metadata": {
        "id": "oRhADBzZghDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Generate synthetic high-dimensional data\n",
        "X, _ = make_classification(\n",
        "    n_samples=500, n_features=20, n_informative=15, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Train PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Step 4: Get explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Step 5: Plot variance captured by each component\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, color='skyblue')\n",
        "plt.plot(range(1, len(explained_variance_ratio) + 1), np.cumsum(explained_variance_ratio), marker='o', color='orange', label='Cumulative Variance')\n",
        "plt.title('Variance Captured by Each Principal Component')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T1s-LSRDf_Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a KNN Classifier and perform feature selection before training."
      ],
      "metadata": {
        "id": "JTbrn7NNhRFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load a real-world dataset (Wine dataset)\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Perform feature selection (select top k features)\n",
        "k = 8  # You can adjust this value\n",
        "selector = SelectKBest(score_func=f_classif, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "# Step 5: Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_selected, y_train)\n",
        "\n",
        "# Step 6: Predict and evaluate\n",
        "y_pred = knn.predict(X_test_selected)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "# Optional: Show selected feature names\n",
        "selected_features = selector.get_support(indices=True)\n",
        "print(\"Selected feature indices:\", selected_features)\n",
        "print(\"Selected feature names:\", data.feature_names)\n"
      ],
      "metadata": {
        "id": "CTKDapwCg4So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. Train a PCA model and visualize the data reconstruction error after reducing dimensions."
      ],
      "metadata": {
        "id": "vDTmYY4Hhx3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Generate synthetic high-dimensional data\n",
        "X, _ = make_classification(\n",
        "    n_samples=500, n_features=20, n_informative=15, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Try different numbers of PCA components and compute reconstruction error\n",
        "components_list = range(1, X_scaled.shape[1] + 1)\n",
        "reconstruction_errors = []\n",
        "\n",
        "for n_components in components_list:\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_reduced = pca.fit_transform(X_scaled)\n",
        "    X_reconstructed = pca.inverse_transform(X_reduced)\n",
        "    error = mean_squared_error(X_scaled, X_reconstructed)\n",
        "    reconstruction_errors.append(error)\n",
        "\n",
        "# Step 4: Plot reconstruction error\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(components_list, reconstruction_errors, marker='o', color='crimson')\n",
        "plt.title('PCA Reconstruction Error vs Number of Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Reconstruction Error (MSE)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TBiUxilPhqsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "47.  Train a KNN Classifier and visualize the decision boundary."
      ],
      "metadata": {
        "id": "MsOf2gTsiyvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Generate a 2D classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=300, n_features=2, n_redundant=0, n_informative=2,\n",
        "    n_clusters_per_class=1, class_sep=1.5, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Train a KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_scaled, y)\n",
        "\n",
        "# Step 4: Create meshgrid for plotting decision boundaries\n",
        "h = 0.02  # step size\n",
        "x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# Step 5: Predict on the meshgrid\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Generate a 2D classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=300, n_features=2, n_redundant=0, n_informative=2,\n",
        "    n_clusters_per_class=1, class_sep=1.5, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Train a KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_scaled, y)\n",
        "\n",
        "# Step 4: Create meshgrid for plotting decision boundaries\n",
        "h = 0.02  # step size\n",
        "x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# Step 5: Predict on the meshgrid\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Step 6: Plot decision boundary and data points\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.4)\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, edgecolors='k')\n",
        "plt.title('Decision Boundary of KNN Classifier')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h1RPBhTLixW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "48.  Train a PCA model and analyze the effect of different numbers of components on data variance."
      ],
      "metadata": {
        "id": "msx7L9O3j-ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Generate high-dimensional synthetic data\n",
        "X, _ = make_classification(\n",
        "    n_samples=500, n_features=20, n_informative=15, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Fit PCA on full data\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Step 4: Extract explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Step 5: Plot cumulative variance vs number of components\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', color='teal')\n",
        "plt.axhline(y=0.9, color='r', linestyle='--', label='90% Variance Threshold')\n",
        "plt.title('Cumulative Explained Variance vs Number of Principal Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RUDkYzi8jyMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1mwzg3rTkRi4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}