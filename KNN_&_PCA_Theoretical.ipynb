{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical"
      ],
      "metadata": {
        "id": "re7ugfARKVyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is K-Nearest Neighbors (KNN) and how does it work ?"
      ],
      "metadata": {
        "id": "1_tcC_GPKPPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans :  K-Nearest Neighbors (KNN) is a simple, versatile machine learning algorithm used for both classification and regression tasks. It works by finding the \"k\" nearest data points in the training set to a new data point and making predictions based on the majority class or average value of those neighbors."
      ],
      "metadata": {
        "id": "XBAL_FIgKnrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between KNN Classification and KNN Regression ?"
      ],
      "metadata": {
        "id": "NEs782HzK7b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : KNN Classification predicts categorical or discrete labels, while KNN Regression predicts continuous numerical values."
      ],
      "metadata": {
        "id": "byB7kLKiLTxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the role of the distance metric in KNN ?"
      ],
      "metadata": {
        "id": "SkOAmke6L5je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : In the k-Nearest Neighbors (k-NN) algorithm, the distance metric quantifies the similarity or dissimilarity between data points, enabling the algorithm to identify the \"nearest neighbors\" and classify or predict based on their labels or values."
      ],
      "metadata": {
        "id": "6ksq-qrbMVmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the Curse of Dimensionality in KNN ?"
      ],
      "metadata": {
        "id": "4GOLP4r6OGbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : In the context of k-Nearest Neighbors (k-NN), the \"curse of dimensionality\" refers to the phenomenon where the performance of the algorithm degrades significantly as the number of features (dimensions) in the data increases, making distances between data points less meaningful and the algorithm less effective."
      ],
      "metadata": {
        "id": "4028V9_OONh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How can we choose the best value of K in KNN ?"
      ],
      "metadata": {
        "id": "rpNTKuknPWQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : To choose the best value of 'k' (number of neighbors) in a k-Nearest Neighbors (kNN) algorithm, you should use techniques like cross-validation, plot error rates against different 'k' values, and consider domain knowledge, aiming for a balance between accuracy and generalization."
      ],
      "metadata": {
        "id": "k7DPKSAuQOBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What are KD Tree and Ball Tree in KNN ?"
      ],
      "metadata": {
        "id": "eY-1Yu1ySTLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : In the context of k-Nearest Neighbors (k-NN) algorithms, KD Trees and Ball Trees are space-partitioning data structures designed to efficiently find the nearest neighbors within a dataset, particularly in high-dimensional spaces. KD Trees partition data along data axes, while Ball Trees organize data based on proximity within hyperspheres."
      ],
      "metadata": {
        "id": "gauapDT5S7zS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should you use KD Tree vs. Ball Tree ?"
      ],
      "metadata": {
        "id": "S4de3EwgTQQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Slower than KD-Trees in low dimensions ( d â‰¤ 3 ) but a lot faster in high dimensions. Both are affected by the curse of dimensionality, but Ball-trees tend to still work if data exhibits local structure"
      ],
      "metadata": {
        "id": "_AzQhK_yT1wW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What are the disadvantages of KNN ?"
      ],
      "metadata": {
        "id": "ogSIj6EbUkre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : KNN, while simple, faces challenges including high computational cost, memory demands, sensitivity to irrelevant features and data scaling, and struggles with high-dimensional datasets and imbalanced classes."
      ],
      "metadata": {
        "id": "VXZUQmBLUrxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How does feature scaling affect KNN ?"
      ],
      "metadata": {
        "id": "Z5Nr4unKWOyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Feature scaling is crucial for K-Nearest Neighbors (KNN) because it ensures that all features contribute equally to the distance calculations, preventing features with larger ranges from dominating the algorithm's decision-making and improving model accuracy."
      ],
      "metadata": {
        "id": "RHyNCdFUWYKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is PCA (Principal Component Analysis) ?"
      ],
      "metadata": {
        "id": "wnRi2t8QWoj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a dataset into a new coordinate system, where the axes, called principal components, are ordered by the amount of variance they explain in the data."
      ],
      "metadata": {
        "id": "KVnUezLkXNIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  How does PCA work ?"
      ],
      "metadata": {
        "id": "xqhTq14sZSPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) reduces the dimensionality of datasets by identifying and retaining the most important information through new, uncorrelated variables called principal components, while discarding less important data."
      ],
      "metadata": {
        "id": "yR9uWIrQZaCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the geometric intuition behind PCA ?"
      ],
      "metadata": {
        "id": "rSTldoZycu6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : PCA geometrically finds the directions (principal components) in the data that capture the most variance, essentially rotating the data to a new coordinate system where the axes are ordered by the amount of variance they explain."
      ],
      "metadata": {
        "id": "eGAwuB9SfdD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the difference between Feature Selection and Feature Extraction ?"
      ],
      "metadata": {
        "id": "r7kZRrd7ffWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Feature selection chooses a subset of the most relevant original features, while feature extraction creates new features from existing ones, aiming to capture hidden patterns or reduce dimensionality."
      ],
      "metadata": {
        "id": "i9-6jJpWfnBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are Eigenvalues and Eigenvectors in PCA ?"
      ],
      "metadata": {
        "id": "9KTLGKs5fm4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : In Principal Component Analysis (PCA), eigenvectors represent the directions (principal components) of the data's variance, while eigenvalues represent the magnitude of that variance along those directions."
      ],
      "metadata": {
        "id": "M-xPTcQegmC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How do you decide the number of components to keep in PCA ?"
      ],
      "metadata": {
        "id": "XcwFt-8ZgxoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : To decide the number of principal components (PCs) to retain in PCA, you can use several methods, including setting a threshold for explained variance, examining a scree plot, or evaluating model performance with different numbers of PCs."
      ],
      "metadata": {
        "id": "X_Ipflgug2ME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Can PCA be used for classification ?"
      ],
      "metadata": {
        "id": "8W-9EE9UhKdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : While Principal Component Analysis (PCA) is primarily a dimensionality reduction technique, and not a classifier itself, you can use PCA as a preprocessing step before applying a classifier for classification tasks, potentially improving performance or reducing computational complexity.\n"
      ],
      "metadata": {
        "id": "jViA0_AzhRmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the limitations of PCA ?"
      ],
      "metadata": {
        "id": "sfGrvmc0h3L5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : PCA (Principal Component Analysis) has several limitations, including the assumption of linear relationships, difficulty in interpreting the new dimensions, sensitivity to outliers, and potential for information loss during dimensionality reduction."
      ],
      "metadata": {
        "id": "PxjveElciCOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 18. How do KNN and PCA complement each other ?"
      ],
      "metadata": {
        "id": "ApW0ECXhiLKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : PCA (Principal Component Analysis) and KNN (k-Nearest Neighbors) complement each other by enabling KNN to work more efficiently and effectively on high-dimensional data by reducing dimensionality through PCA, which in turn helps KNN to classify data more accurately and quickly."
      ],
      "metadata": {
        "id": "-lTP17XJigVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How does KNN handle missing values in a dataset ?"
      ],
      "metadata": {
        "id": "jtii8LCnj5ii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Imputation with K-Nearest Neighbors (KNN) estimates missing values in a dataset by considering the values of the closest data points, determined by a distance metric like Euclidean distance. The missing value is then assigned the average of these nearest neighbors' values, weighted by their proximity."
      ],
      "metadata": {
        "id": "LYNiPN2Gj-UB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are the key differences between PCA and Linear Discriminant Analysis (LDA) ?"
      ],
      "metadata": {
        "id": "8NUx15QBkGcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : The key difference between PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis) lies in their supervised/unsupervised nature and objective: PCA is unsupervised, aiming to find directions of maximum variance, while LDA is supervised, focusing on maximizing class separability."
      ],
      "metadata": {
        "id": "iu8zwWXekQWc"
      }
    }
  ]
}